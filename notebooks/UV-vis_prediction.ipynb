{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from dataclasses import dataclass\n",
    "from pymatgen.core.periodic_table import Element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMILES, corresponding weight and Temp info\n",
    "smiles    = ['!','\\_...','\\_...',...] \\\n",
    "weight    = [1,...] \\\n",
    "temp_info = [injection Temp, increasing speed, time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_smiles_detailed(smiles):\n",
    "    result = []\n",
    "    len_smiles = len(smiles)\n",
    "    n = 0\n",
    "    while n < len_smiles:\n",
    "        if smiles[n].isalpha():\n",
    "            if n+1 < len_smiles:\n",
    "                if smiles[n].isupper() and smiles[n+1].islower():\n",
    "                    try:\n",
    "                        atom = Element(smiles[n:n+2])\n",
    "                        result.append(smiles[n:n+2])\n",
    "                        n += 2\n",
    "                    except:\n",
    "                        result.append(smiles[n])\n",
    "                        result.append(smiles[n+1])\n",
    "                        n += 2\n",
    "                else:\n",
    "                    result.append(smiles[n])\n",
    "                    n += 1\n",
    "            else:\n",
    "                result.append(smiles[n])\n",
    "                n += 1\n",
    "        else:\n",
    "            result.append(smiles[n])\n",
    "            n += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = []\n",
    "weight_smiles = []\n",
    "temp_info = []\n",
    "idx_train = []\n",
    "idx_test = []\n",
    "idx_tot = []\n",
    "with open('../data/input_SMILES.txt','r') as f:\n",
    "    i = 0\n",
    "    while True:\n",
    "        tmp = f.readline()\n",
    "        if len(tmp) == 0:\n",
    "            break\n",
    "        tmp = tmp.split()\n",
    "        temp_info.append([float(x) for x in tmp[-5:-1]])\n",
    "        smiles.append(['!']+list(map(lambda x: '_' + x ,tmp[:-5][::2])))\n",
    "        weight_smiles.append([1.0]+[float(x) for x in tmp[:-5][1::2]])\n",
    "        if tmp[-1] == 'T':\n",
    "            idx_train.append(i)\n",
    "            idx_tot.append(i)\n",
    "        elif tmp[-1] == 'F':\n",
    "            idx_test.append(i)\n",
    "            idx_tot.append(i)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_vocab = set()\n",
    "MAX_LENGTH_SMILES = 1\n",
    "for i in range(len(smiles)):\n",
    "    for j in range(len(smiles[i])):\n",
    "        if MAX_LENGTH_SMILES < len(smiles[i][j]):\n",
    "            MAX_LENGTH_SMILES = len(smiles[i][j])\n",
    "        for char in split_smiles_detailed(smiles[i][j]):\n",
    "            smiles_vocab.add(char)\n",
    "smiles_vocab = sorted(list(smiles_vocab))\n",
    "smiles_to_index = dict([(char), i+1] for i, char in enumerate(smiles_vocab))\n",
    "SMILES_VOCAB_SIZE = len(smiles_to_index)+1\n",
    "print(smiles_to_index)\n",
    "print(SMILES_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    MAX_LENGTH_SMILES = MAX_LENGTH_SMILES\n",
    "    EMBED_DIM = 256\n",
    "    SMILES_ATT_NUM_HEADS = 4\n",
    "    SMILES_ATT_DFF = EMBED_DIM*4\n",
    "    SMILES_VOCAB_SIZE = SMILES_VOCAB_SIZE\n",
    "    CHEMICAL_ATT_NUM_HEADS = 4\n",
    "    CHEMICAL_ATT_DFF = EMBED_DIM*4\n",
    "    CHEMICAL_TRANSFORMER_NUM_LAYERS = 3\n",
    "    PROCESS_ATT_NUM_HEADS = 1\n",
    "    PROCESS_ATT_DFF = EMBED_DIM*4\n",
    "    PROCESS_TRANSFORMER_NUM_LAYERS = 3\n",
    "    LR = 1e-4\n",
    "    BATCH_SIZE = 512\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_max = np.max(list(map(lambda x: len(x), smiles)))\n",
    "n_tot_data = len(idx_tot)\n",
    "print(f'Total data               : {n_tot_data}')\n",
    "print(f'Maximum length           : {len_max}')\n",
    "print(f'Maximum length of smiles : {config.MAX_LENGTH_SMILES}')\n",
    "print(f'Vocab size               : {config.SMILES_VOCAB_SIZE}')\n",
    "\n",
    "np.random.shuffle(idx_test)\n",
    "np.random.shuffle(idx_train)\n",
    "\n",
    "print(f'Size of dataset : {n_tot_data}')\n",
    "print(f'Train size      : {len(idx_train)}')\n",
    "print(f'Test_size       : {len(idx_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert string to numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_array = np.zeros([len(smiles),len_max,config.MAX_LENGTH_SMILES]) # if there is no data, the array will be zero.\n",
    "weight_array  = np.zeros([len(weight_smiles),len_max])\n",
    "temp_info     = np.array(temp_info)[:,[0,-1]]\n",
    "for i in range(len(smiles)):\n",
    "    for j in range(len(smiles[i])):\n",
    "        weight_array[i,j] = weight_smiles[i][j]\n",
    "        for k, char in enumerate(split_smiles_detailed(smiles[i][j])):\n",
    "            encoder_array[i,j,k] = smiles_to_index[char]\n",
    "dim_X = encoder_array.shape[1:]\n",
    "print(f'dim_X : {dim_X}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_smiles_to_index = {v: k for k, v in smiles_to_index.items()}\n",
    "\n",
    "def decoding_smiles(encoder_array,inv_smiles_to_index):\n",
    "    tot_smiles = []\n",
    "    for i in range(encoder_array.shape[0]):\n",
    "        if encoder_array[i,0] == 0:\n",
    "            break\n",
    "        else:\n",
    "            smiles = ''\n",
    "        for j in range(encoder_array.shape[1]-1):\n",
    "            if encoder_array[i,j+1] == 0:\n",
    "                break\n",
    "            else:\n",
    "                smiles += inv_smiles_to_index[int(encoder_array[i,j+1])]\n",
    "        tot_smiles.append(smiles)\n",
    "    return tot_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Y (Absorbance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = np.load('../data/Absorbances.npy')[:,:,1]\n",
    "dim_target = target.shape[1:]\n",
    "print(dim_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test, Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train        = tf.cast(encoder_array[idx_train],tf.float32)\n",
    "X_weight_train = tf.cast(weight_array[idx_train],tf.float32)\n",
    "X_temp_train   = tf.cast(temp_info[idx_train],tf.float32)\n",
    "y_train        = target[idx_train]\n",
    "\n",
    "X_test        = tf.cast(encoder_array[idx_test],tf.float32)\n",
    "X_weight_test = tf.cast(weight_array[idx_test],tf.float32)\n",
    "X_temp_test   = tf.cast(temp_info[idx_test],tf.float32)\n",
    "y_test        = target[idx_test]\n",
    "\n",
    "X_tot        = tf.cast(encoder_array[idx_tot],tf.float32)\n",
    "X_weight_tot = tf.cast(weight_array[idx_tot],tf.float32)\n",
    "X_temp_tot   = tf.cast(temp_info[idx_tot],tf.float32)\n",
    "y_tot        = target[idx_tot]\n",
    "\n",
    "print('Training data information')\n",
    "print(f'X_train        = {X_train.shape}')\n",
    "print(f'X_weight_train = {X_weight_train.shape}')\n",
    "print(f'X_temp_train   = {X_temp_train.shape}')\n",
    "print(f'y_train        = {y_train.shape}')\n",
    "\n",
    "print('Test data information')\n",
    "print(f'X_test        = {X_test.shape}')\n",
    "print(f'X_weight_test = {X_weight_test.shape}')\n",
    "print(f'X_temp_test   = {X_temp_test.shape}')\n",
    "print(f'y_test        = {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer based UV-vis prediction model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLearningRateSchedule(keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, warmup_steps, initial_lr, peak_lr, tot_steps,n_epoch,**kwargs):\n",
    "        super(CustomLearningRateSchedule, self).__init__(**kwargs)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.initial_lr   = initial_lr\n",
    "        self.peak_lr      = peak_lr\n",
    "        self.tot_steps   = tot_steps\n",
    "        self.n_epoch    = n_epoch\n",
    "    def __call__(self, step):\n",
    "        # Linear warmup\n",
    "        warmup_lr = self.initial_lr + (self.peak_lr - self.initial_lr) * (step / self.warmup_steps)\n",
    "        # decay_lr  = self.peak_lr *(1 - (step-self.warmup_steps)/(self.tot_steps*self.n_epoch-self.warmup_steps))\n",
    "        learning_rate = tf.cond(\n",
    "            step < self.warmup_steps,\n",
    "            lambda: tf.cast(warmup_lr, dtype=tf.float32),\n",
    "            lambda: tf.cast(self.peak_lr,dtype=tf.float32)\n",
    "        )\n",
    "        return learning_rate\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"warmup_steps\" : self.warmup_steps,\n",
    "                \"initial_lr\" : self.initial_lr,\n",
    "                \"peak_lr\" : self.peak_lr,\n",
    "                \"tot_steps\" : self.tot_steps,\n",
    "                \"n_epoch\" : self.n_epoch,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Recreate the schedule from its config\n",
    "        return cls(**config)\n",
    "\n",
    "class RoPE(keras.layers.Layer):\n",
    "    def __init__(self, dim, **kwargs):\n",
    "        super(RoPE, self).__init__(**kwargs)\n",
    "        self.dim = dim\n",
    "        self.inv_freq = 1. / (10000 ** (tf.range(0, dim, 2.0) / dim))\n",
    "\n",
    "    def call(self, inputs_even,inputs_odd, positions=None):\n",
    "        # Determine the sequence length\n",
    "        seq_len = tf.shape(inputs_even)[1]\n",
    "        # Compute positions if not provided\n",
    "        if positions is None:\n",
    "            positions = tf.range(seq_len, dtype=tf.float32)\n",
    "        # \n",
    "        # Compute sinusoidal embeddings\n",
    "        freqs = tf.einsum('i,j->ij', positions, self.inv_freq) # Shape : (seq_len, dim/2)\n",
    "        cos = tf.cos(freqs) # Shape : (seq_len, dim/2)\n",
    "        sin = tf.sin(freqs) # Shape : (seq_len, dim/2)\n",
    "        # \n",
    "        # Split input into even and odd dimensions for rotation\n",
    "        # \n",
    "        # Apply RoPE rotaiton\n",
    "        x_rotated_even = inputs_even * cos - inputs_odd * sin\n",
    "        X_rotated_odd  = inputs_even * sin + inputs_odd * cos\n",
    "        # \n",
    "        # Interleave the even and odd dimensions back together\n",
    "        x_rotated = tf.reshape(\n",
    "            tf.concat([x_rotated_even, X_rotated_odd], axis=-1), (-1, seq_len, self.dim)\n",
    "        )\n",
    "        return x_rotated\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            \"dim\": self.dim,\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        # Recreate the schedule from its config\n",
    "        return cls(**config)\n",
    "\n",
    "class MaskProcessor(keras.layers.Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MaskProcessor, self).__init__(**kwargs)\n",
    "    \n",
    "    def call(self, padding_mask):\n",
    "        padding_mask = tf.cast(padding_mask, tf.float32)\n",
    "        attention_mask = padding_mask[:,:,tf.newaxis] * padding_mask[:,tf.newaxis,:]\n",
    "        \n",
    "        return attention_mask\n",
    "\n",
    "def chemical_bert_module(query, key, value, attention_mask, i):\n",
    "    # Multi headed self-attention\n",
    "    assert config.EMBED_DIM % config.CHEMICAL_ATT_NUM_HEADS == 0\n",
    "    attention_output, attention_scores = keras.layers.MultiHeadAttention(\n",
    "        num_heads=config.CHEMICAL_ATT_NUM_HEADS,\n",
    "        key_dim=config.EMBED_DIM // config.CHEMICAL_ATT_NUM_HEADS,\n",
    "        name=f\"ChemENC_{i}_multiheadattention\",\n",
    "    )(query, value, key, attention_mask=attention_mask, return_attention_scores=True)\n",
    "    attention_output = keras.layers.Dropout(0.1,name=f\"ChemENC_{i}_att_dropout\")(attention_output)\n",
    "    attention_output = keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"ChemENC_{i}_att_layernormalization\"\n",
    "    )(value + attention_output)\n",
    "    \n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(config.CHEMICAL_ATT_DFF, activation='silu'),\n",
    "            keras.layers.Dense(config.EMBED_DIM)    \n",
    "        ],\n",
    "        name=f\"ChemENC_{i}_ffn\"\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = keras.layers.Dropout(0.1,name=f\"ChemENC_{i}_ffn_dropout\")(ffn_output)\n",
    "    sequence_output = keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"ChemENC_{i}_ffn_layernormalization\"\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "def process_bert_module(query, key, value, attention_mask, i):\n",
    "    # Multi headed self-attention\n",
    "    assert (config.EMBED_DIM) % config.PROCESS_ATT_NUM_HEADS == 0\n",
    "    attention_output, attention_scores = keras.layers.MultiHeadAttention(\n",
    "        num_heads=config.PROCESS_ATT_NUM_HEADS,\n",
    "        key_dim=(config.EMBED_DIM) // config.PROCESS_ATT_NUM_HEADS,\n",
    "        name=f\"ProcENC_{i}_multiheadattention\",\n",
    "    )(query, value, key, attention_mask=attention_mask, return_attention_scores=True)\n",
    "    attention_output = keras.layers.Dropout(0.1,name=f\"ProcENC_{i}_att_dropout\")(attention_output)\n",
    "    attention_output = keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"ProcENC_{i}_att_layernormalization\"\n",
    "    )(value + attention_output)\n",
    "    # Feed-forward layer\n",
    "    ffn = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Dense(config.PROCESS_ATT_DFF, activation='silu'),\n",
    "            keras.layers.Dense(config.EMBED_DIM)    \n",
    "        ],\n",
    "        name=f\"ProcENC_{i}_ffn\"\n",
    "    )\n",
    "    ffn_output = ffn(attention_output)\n",
    "    ffn_output = keras.layers.Dropout(0.1,name=f\"ProcENC_{i}_ffn_dropout\")(ffn_output)\n",
    "    sequence_output = keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6, name=f\"ProcENC_{i}_ffn_layernormalization\"\n",
    "    )(attention_output + ffn_output)\n",
    "    return sequence_output\n",
    "\n",
    "class Expanded_matrix(keras.layers.Layer):\n",
    "    def __init__(self, embedding_dim,*args,**kwargs):\n",
    "        super(Expanded_matrix,self).__init__(*args,**kwargs)\n",
    "        self.embedding_dim = embedding_dim\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        expanded_m = tf.expand_dims(inputs,axis=-1)\n",
    "        outputs = tf.tile(expanded_m,[1,1,self.embedding_dim])\n",
    "        return outputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embedding_dim\" : self.embedding_dim\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)\n",
    "\n",
    "class ReshapeLayer(keras.layers.Layer):\n",
    "    def __init__(self, shape,*args,**kwargs):\n",
    "        super(ReshapeLayer, self).__init__(*args,**kwargs)\n",
    "        self.shape = shape\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x_reshaped = tf.reshape(inputs, self.shape)\n",
    "        return x_reshaped\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"shape\" : self.shape\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_smiles = keras.layers.Input(shape=dim_X,name='input_smiles') # (None, length_process, lenght_smiles)\n",
    "input_weight = keras.layers.Input(shape=dim_X[:-1],name='input_weight') #(None, length_process)\n",
    "input_temp = keras.layers.Input(shape=(X_temp_train.shape[-1],),name='input_temp') #(None,3)\n",
    "\n",
    "reshape_smiles = ReshapeLayer(shape=(-1,dim_X[1]))\n",
    "reshape_process = ReshapeLayer(shape=(-1,dim_X[0],config.EMBED_DIM))\n",
    "embedding_layer = keras.layers.Embedding(input_dim=config.SMILES_VOCAB_SIZE, output_dim=config.EMBED_DIM,name='embed_layer') # (None*length_process,length_smiles) -> (None*length_process,length_smiles,config.EMBED_DIM)\n",
    "embed_weight = Expanded_matrix(config.EMBED_DIM) # (None,length_process) -> (None,length_process,config.WEIGHT_DIM)\n",
    "\n",
    "x = input_smiles\n",
    "process_padding_mask = keras.layers.Embedding(len_max,config.EMBED_DIM,mask_zero=True,trainable=False)(x[:,:,0])._keras_mask\n",
    "process_attention_mask = MaskProcessor()(process_padding_mask)\n",
    "x = reshape_smiles(input_smiles)\n",
    "chemical_padding_mask = keras.layers.Embedding(config.SMILES_VOCAB_SIZE,config.EMBED_DIM,mask_zero=True,trainable=False)(x)._keras_mask\n",
    "chemical_attention_mask = MaskProcessor()(chemical_padding_mask)\n",
    "\n",
    "x = embedding_layer(x)\n",
    "x *= tf.math.sqrt(tf.cast(config.EMBED_DIM, tf.float32))\n",
    "for i in range(config.CHEMICAL_TRANSFORMER_NUM_LAYERS):\n",
    "    query = RoPE(config.EMBED_DIM)(x[:,:,0::2],x[:,:,1::2])\n",
    "    key   = RoPE(config.EMBED_DIM)(x[:,:,0::2],x[:,:,1::2])\n",
    "    x = chemical_bert_module(query,key,x,chemical_attention_mask,i)\n",
    "x = x[:,0,:] # Pick the class token of smiles input (BATCH,MAX_LENGTH_SMILES,EMBED_DIM) -> (BATCH,EMBED_DIM)\n",
    "\n",
    "\n",
    "x = reshape_process(x) # (BATCH,EMBED_DIM) -> (BATCH,length_process,EMBED_DIM)\n",
    "x_weight = embed_weight(input_weight)\n",
    "scaling_weight = keras.layers.Dense(1,activation='softplus',name='WeightScale')(tf.ones((1,1), dtype=tf.float32)) # Trainable scaling layer for weight information\n",
    "x_weight = x_weight * scaling_weight\n",
    "x = x * x_weight\n",
    "for i in range(config.PROCESS_TRANSFORMER_NUM_LAYERS):\n",
    "    query = RoPE(config.EMBED_DIM)(x[:,:,0::2],x[:,:,1::2])\n",
    "    key   = RoPE(config.EMBED_DIM)(x[:,:,0::2],x[:,:,1::2])\n",
    "    x = process_bert_module(query,key,x,process_attention_mask,i)\n",
    "x = x[:,0,:]\n",
    "\n",
    "scaling_temp = keras.layers.Dense(X_temp_train.shape[-1],activation='sigmoid',name='TempScale')(tf.ones((1,1), dtype=tf.float32)) # Trainable scaling layer for temperature information\n",
    "x_temp = input_temp * scaling_temp\n",
    "x = keras.layers.Concatenate(axis=-1)([x,x_temp])\n",
    "x = keras.layers.Dense(dim_target[0]*8, activation='silu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "x = keras.layers.Dense(dim_target[0]*4, activation='silu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "output = keras.layers.Dense(dim_target[0], activation='softplus')(x)\n",
    "\n",
    "model = keras.Model(inputs=[input_smiles,input_weight,input_temp],outputs=[output])\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedule = CustomLearningRateSchedule(\n",
    "    warmup_steps=5000,\n",
    "    initial_lr=0.0,\n",
    "    peak_lr=config.LR,\n",
    "    tot_steps=np.ceil(len(X_train)/config.BATCH_SIZE),\n",
    "    n_epoch=100000\n",
    ")\n",
    "# opt = tf.keras.optimizers.Adam(learning_rate=config.LR)\n",
    "opt = tf.keras.optimizers.AdamW(learning_rate=lr_schedule,beta_1=0.9,beta_2=0.999,weight_decay=0.01)\n",
    "# opt = tf.keras.optimizers.AdamW(learning_rate=5e-5,beta_1=0.9,beta_2=0.999,weight_decay=0.01)\n",
    "model.compile(\n",
    "    optimizer=opt,\n",
    "    loss=tf.keras.losses.MeanSquaredError(reduction='sum_over_batch_size', name='mean_squared_error'),\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath='UVvis_attention_model.keras', monitor='val_loss', verbose=2, save_best_only=True, mode='min', initial_value_threshold=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train,X_weight_train,X_temp_train], y=y_train, epochs=30000, batch_size=512, verbose=0, validation_data=([X_test,X_weight_test,X_temp_test],y_test), callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-trained parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = keras.models.load_model(\n",
    "    '../pre-trained_model/UVvis_attention_model.keras',custom_objects={\n",
    "        \"CustomLearningRateSchedule\":CustomLearningRateSchedule,\n",
    "        \"RoPE\":RoPE,\n",
    "        \"ReshapeLayer\":ReshapeLayer,\n",
    "        \"Expanded_matrix\":Expanded_matrix,\n",
    "        \"MaskProcessor\":MaskProcessor,\n",
    "        }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
